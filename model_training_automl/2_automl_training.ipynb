{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Train an Automated Machine Learning Model for Trash Recycling using Instance Segmentation\n",
        "\n",
        "This notebook is an adaptation from: https://github.com/Azure/azureml-examples/tree/main/sdk/python/jobs/automl-standalone-jobs/automl-image-instance-segmentation-task-fridge-items .\n",
        "Please keep an eye on this repo as is contains the most recent examples of the SDK v2 usage and is subject to change.\n",
        "\n",
        "<img src=\"media/trash_segmented.png\" width=\"800\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1664338518997
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch import nn\n",
        "import os\n",
        "\n",
        "# Import required libraries\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.ml import MLClient\n",
        "\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from azure.ai.ml import Input\n",
        "from azure.ai.ml.automl import ImageClassificationSearchSpace\n",
        "from azure.ai.ml.sweep import (\n",
        "    Choice,\n",
        "    Uniform,\n",
        "    BanditPolicy,\n",
        ")\n",
        "# import required libraries\n",
        "from azure.ai.ml.entities import (\n",
        "    ManagedOnlineEndpoint,\n",
        "    ManagedOnlineDeployment,\n",
        "    Model,\n",
        "    Environment,\n",
        "    CodeConfiguration,\n",
        "    ProbeSettings,\n",
        ")\n",
        "\n",
        "from azure.ai.ml.entities import Data\n",
        "from azure.ai.ml.constants import InputOutputModes\n",
        "from azure.ai.ml import automl\n",
        "\n",
        "# IMPORT CUSTOM FUNCTION LIBRARY\n",
        "from utils import *\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 1. Setup of ML Client\n",
        "\n",
        "The ML client is the SDK object that allows us later to train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1664338526342
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "credential = DefaultAzureCredential()\n",
        "ml_client = None\n",
        "\n",
        "try:\n",
        "    ml_client = MLClient.from_config(credential=credential)\n",
        "except Exception as ex:\n",
        "    # NOTE: Update following workspace information if not correctly configure before\n",
        "    client_config = {\n",
        "        \"subscription_id\": \"<SUBSCRIPTION ID>\",\n",
        "        \"resource_group\": \"<RG_NAME>\",\n",
        "        \"workspace_name\": \"<WORKSPACENAME>\",\n",
        "    }\n",
        "    if client_config[\"subscription_id\"].startswith(\"<\"):\n",
        "        print(\n",
        "            \"please update your <SUBSCRIPTION_ID> <RESOURCE_GROUP> <AML_WORKSPACE_NAME> in notebook cell\"\n",
        "        )\n",
        "        raise ex\n",
        "    else:  # write and reload from config file\n",
        "        import json, os\n",
        "        config_path = \"../.azureml/config.json\"\n",
        "        os.makedirs(os.path.dirname(config_path), exist_ok=True)\n",
        "        with open(config_path, \"w\") as fo:\n",
        "            fo.write(json.dumps(client_config))\n",
        "        ml_client = MLClient.from_config(credential=credential, path=config_path)\n",
        "print(ml_client)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 2. Load pre-processed data \n",
        "\n",
        "We will now access the datasets that we have prepared for model training which can also be found in the data tile. You can also see the versions of the registered dataset in the UI to understand better which data to access."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1664338714903
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml import Input\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "\n",
        "# Training MLTable defined locally, with local data to be uploaded\n",
        "#my_training_data_input = Input(type=AssetTypes.MLTABLE, path=training_mltable_path)\n",
        "# Validation MLTable defined locally, with local data to be uploaded\n",
        "#my_validation_data_input = Input(type=AssetTypes.MLTABLE, path=validation_mltable_path)\n",
        "\n",
        "# WITH REMOTE PATH: If available already in the cloud/workspace-blob-store. \n",
        "#If you need a specific version, adjust the path to \"train_trash_jsonl:<version_number>\" denotes the file version\n",
        "\n",
        "my_training_data_input = Input(\n",
        "    type=AssetTypes.MLTABLE, path=\"azureml:train_trash_jsonl:1\",\n",
        "    mode=InputOutputModes.DIRECT\n",
        ")\n",
        "\n",
        "# Validation MLTable with versioned TabularDataset\n",
        "my_validation_data_input = Input(\n",
        "    type=AssetTypes.MLTABLE, path=\"azureml:val_trash_jsonl:1\",\n",
        "    mode=InputOutputModes.DIRECT\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 3. Compute target setup\n",
        "\n",
        "You will need to provide a Compute Target that will be used for your AutoML model training. AutoML models for image tasks require GPU SKUs such as the ones from the NC, NCv2, NCv3, ND, NDv2 and NCasT4 series. We recommend using the NCsv3-series (with v100 GPUs) for faster training. Using a compute target with a multi-GPU VM SKU will leverage the multiple GPUs to speed up training. Additionally, setting up a compute target with multiple nodes will allow for faster model training by leveraging parallelism, when tuning hyperparameters for your model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1664338637732
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azure.ai.ml.entities import AmlCompute\n",
        "from azure.core.exceptions import ResourceNotFoundError\n",
        "\n",
        "compute_name = \"gpu-cluster\"\n",
        "\n",
        "try:\n",
        "    _ = ml_client.compute.get(compute_name)\n",
        "    print(\"Found existing compute target.\")\n",
        "except ResourceNotFoundError:\n",
        "    print(\"Creating a new compute target...\")\n",
        "    compute_config = AmlCompute(\n",
        "        name=compute_name,\n",
        "        type=\"amlcompute\",\n",
        "        size=\"Standard_NC6\",\n",
        "        idle_time_before_scale_down=120,\n",
        "        min_instances=0,\n",
        "        max_instances=4,\n",
        "    )\n",
        "    ml_client.begin_create_or_update(compute_config)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "```\n",
        "# For tuning task on seperate cluster \n",
        "\n",
        "compute_name_tuning = \"gpu-cluster-tuning\"\n",
        "try:\n",
        "    _ = ml_client.compute.get(compute_name_tuning)\n",
        "    print(\"Found existing compute target.\")\n",
        "except ResourceNotFoundError:\n",
        "    print(\"Creating a new compute target...\")\n",
        "    compute_config = AmlCompute(\n",
        "        name=compute_name_tuning,\n",
        "        type=\"amlcompute\",\n",
        "        size=\"Standard_NC6\",\n",
        "        idle_time_before_scale_down=120,\n",
        "        min_instances=0,\n",
        "        max_instances=4,\n",
        "    )\n",
        "    ml_client.begin_create_or_update(compute_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 4. Configure and run the AutoML for Images Instance Segmentation training job\n",
        "\n",
        "AutoML allows you to easily train models for Image Classification, Object Detection & Instance Segmentation on your image data. You can control the model algorithm to be used, specify hyperparameter values for your model as well as perform a sweep across the hyperparameter space to generate an optimal model.\n",
        "\n",
        "When using AutoML for image tasks, you need to specify the model algorithms using the model_name parameter. You can either specify a single model or choose to sweep over multiple models. Please refer to the [documentation](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-image-models?tabs=CLI-v2#configure-model-algorithms-and-hyperparameters) for the list of supported model algorithms.\n",
        "\n",
        "### 4.1. Using default hyperparameter values for the specified algorithm\n",
        "Before doing a large sweep to search for the optimal models and hyperparameters, we recommend trying the default values for a given model to get a first baseline. Next, you can explore multiple hyperparameters for the same model before sweeping over multiple models and their parameters. This allows an iterative approach, as with multiple models and multiple hyperparameters for each (as we showcase in the next section), the search space grows exponentially, and  you need more iterations to find optimal configurations.\n",
        "\n",
        "Following functions are used to configure the AutoML image job:\n",
        "#### image_instance_segmentation() function parameters:\n",
        "The `image_instance_segmentation()` factory function allows user to configure the training job.\n",
        "\n",
        "- `compute` - The compute on which the AutoML job will run. In this example we are using a compute called 'gpu-cluster' present in the workspace. You can replace it any other compute in the workspace.\n",
        "- `experiment_name` - The name of the experiment. An experiment is like a folder with multiple runs in Azure ML Workspace that should be related to the same logical machine learning experiment.\n",
        "- `name` - The name of the Job/Run. This is an optional property. If not specified, a random name will be generated.\n",
        "- `primary_metric` - The metric that AutoML will optimize for model selection.\n",
        "- `target_column_name` - The name of the column to target for predictions. It must always be specified. This parameter is applicable to 'training_data' and 'validation_data'.\n",
        "- `training_data` - The data to be used for training. It should contain both training feature columns and a target column. Optionally, this data can be split for segregating a validation or test dataset. \n",
        "You can use a registered MLTable in the workspace using the format '<mltable_name>:<version>' OR you can use a local file or folder as a MLTable. For e.g Input(mltable='my_mltable:1') OR Input(mltable=MLTable(local_path=\"./data\"))\n",
        "The parameter 'training_data' must always be provided.\n",
        "\n",
        "#### set_limits() parameters:\n",
        "This is an optional configuration method to configure limits parameters such as timeouts.     \n",
        "    \n",
        "- `timeout_minutes` - Maximum amount of time in minutes that the whole AutoML job can take before the job terminates. If not specified, the default job's total timeout is 6 days (8,640 minutes).\n",
        "\n",
        "#### set_image_model() function parameters:\n",
        "This is an optional configuration method to configure fixed settings or parameters that don't change during the parameter space sweep. Some of the key parameters of this function are:\n",
        "\n",
        "- `model_name` - The name of the ML algorithm that we want to use in training job. Please refer to this [documentation](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-image-models?tabs=CLI-v2#supported-model-algorithms) for supported model algorithm.\n",
        "- `number_of_epochs` - The number of training epochs. It must be positive integer (default value is 15).\n",
        "- `layers_to_freeze` - The number of layers to freeze in model for transfer learning. It must be a positive integer (default value is 0).\n",
        "- `early_stopping` - It enable early stopping logic during training, It must be boolean value (default is True).   \n",
        "- `optimizer` - Type of optimizer to use in training. It must be either sgd, adam, adamw (default is sgd).\n",
        "- `distributed` - It enable distributed training if compute target contain multiple GPUs. It must be boolean value (default is True).\n",
        "\n",
        "If you wish to use the default hyperparameter values for a given algorithm (say `maskrcnn`). For more info, check: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-image-models?tabs=python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1664338674444
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# general job parameters\n",
        "exp_name = \"trash-image-instance-segmentation-experiment_tutorial\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1664338726080
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Create the AutoML job with the related factory-function.\n",
        "\n",
        "image_instance_segmentation_job = automl.image_instance_segmentation(\n",
        "    compute=compute_name,\n",
        "    experiment_name=exp_name,\n",
        "    training_data=my_training_data_input,\n",
        "    validation_data=my_validation_data_input,\n",
        "    target_column_name=\"label\",\n",
        ")\n",
        "\n",
        "image_instance_segmentation_job.set_limits(timeout_minutes=700) #The larger the dataset, the larger the timeout. Since we work on a subset, 120 minutes should suffice\n",
        "\n",
        "image_instance_segmentation_job.set_image_model(model_name=\"maskrcnn_resnet50_fpn\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 5. Submitting an AutoML job for Computer Vision tasks\n",
        "Once you've configured your job, you can submit it as a job in the workspace in order to train a vision model using your training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Submit the AutoML job\n",
        "returned_job = ml_client.jobs.create_or_update(image_instance_segmentation_job)\n",
        "\n",
        "print(f\"Created job: {returned_job}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1664339001806
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "ml_client.jobs.stream(returned_job.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 6. Hyperparameter sweeping for your AutoML models for computer vision tasks\n",
        "\n",
        "When using AutoML for Images, you can perform a hyperparameter sweep over a defined parameter space to find the optimal model. In this example, we sweep over the hyperparameters for `maskrcnn_resnet50_fpn` model which is pretrained on COCO, a large-scale object detection, segmentation, and captioning dataset that contains over 200K labeled images with over 80 label categories, choosing from a range of values for learning_rate, optimizer, etc., to generate a model with the optimal 'MeanAveragePrecision'. If hyperparameter values are not specified, then default values are used for the specified algorithm.\n",
        "\n",
        "set_sweep function is used to configure the sweep settings:\n",
        "### set_sweep() parameters:\n",
        "\n",
        "- `max_trials` - Required parameter for maximum number of configurations to sweep. Must be an integer between 1 and 1000. When exploring just the default hyperparameters for a given model algorithm, set this parameter to 1.\n",
        "- `max_concurrent_trials` - Maximum number of runs that can run concurrently. If not specified, all runs launch in parallel. If specified, must be an integer between 1 and 100.\n",
        "    NOTE: The number of concurrent runs is gated on the resources available in the specified compute target. Ensure that the compute target has the available resources for the desired concurrency.\n",
        "- `sampling_algorithm` - Sampling method to use for sweeping over the defined parameter space. Please refer to this [documentation](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-image-models?tabs=SDK-v2#sampling-methods-for-the-sweep) for list of supported sampling methods.\n",
        "- `early_termination` - Early termination policy to end poorly performing runs. If no termination policy is specified, all configurations are run to completion. Please refer to this [documentation](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-auto-train-image-models?tabs=SDK-v2#early-termination-policies) for supported early termination policies.\n",
        "\n",
        "We use Random Sampling to pick samples from this parameter space and try a total of 10 iterations with these different samples, running 2 iterations at a time on our compute target. Please note that the more parameters the space has, the more iterations you need to find optimal models.\n",
        "\n",
        "We leverage the Bandit early termination policy which will terminate poor performing configs (those that are not within 20% slack of the best performing config), thus significantly saving compute resources.\n",
        "\n",
        "For more details on model and hyperparameter sweeping, please refer to the [documentation](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1663631699911
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Create the AutoML job with the related factory-function.\n",
        "from azure.ai.ml.automl import ImageObjectDetectionSearchSpace\n",
        "\n",
        "image_instance_segmentation_job = automl.image_instance_segmentation(\n",
        "    compute=compute_name_tuning,\n",
        "    experiment_name=\"trash_hyperparameter_job\",\n",
        "    training_data=my_training_data_input,\n",
        "    validation_data=my_validation_data_input,\n",
        "    target_column_name=\"label\",\n",
        "    primary_metric=\"MeanAveragePrecision\",\n",
        "    tags={\"hackathon\": \"instance_segmentation\"},\n",
        ")\n",
        "\n",
        "image_instance_segmentation_job.set_limits(timeout_minutes=500)\n",
        "\n",
        "image_instance_segmentation_job.extend_search_space(\n",
        "    [\n",
        "        ImageObjectDetectionSearchSpace(\n",
        "            model_name=Choice([\"maskrcnn_resnet50_fpn\"]),\n",
        "            learning_rate=Uniform(0.0001, 0.001),\n",
        "            # warmup_cosine_lr_warmup_epochs=Choice([0, 3]),\n",
        "            optimizer=Choice([\"sgd\", \"adam\", \"adamw\"]),\n",
        "            min_size=Choice([600, 800]),\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "image_instance_segmentation_job.set_sweep(\n",
        "    max_trials=10,\n",
        "    max_concurrent_trials=2,\n",
        "    sampling_algorithm=\"Random\",\n",
        "    early_termination=BanditPolicy(\n",
        "        evaluation_interval=2, slack_factor=0.2, delay_evaluation=6\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1663631706680
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Submit the AutoML job\n",
        "returned_job = ml_client.jobs.create_or_update(\n",
        "    image_instance_segmentation_job\n",
        ")  # submit the job to the backend\n",
        "\n",
        "print(f\"Created job: {returned_job}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1663631728506
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "hd_job = ml_client.jobs.get(returned_job.name + \"_HD\")\n",
        "hd_job"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 7. Initialize MLFlow Client for tracking\n",
        "\n",
        "The models and artifacts that are produced by AutoML can be accessed via the MLFlow interface.\n",
        "Initialize the MLFlow client here, and set the backend as Azure ML, via. the MLFlow Client.\n",
        "\n",
        "IMPORTANT, you need to have installed the latest MLFlow packages with:\n",
        "\n",
        "    pip install azureml-mlflow\n",
        "\n",
        "    pip install mlflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "\n",
        "# Obtain the tracking URL from MLClient\n",
        "MLFLOW_TRACKING_URI = ml_client.workspaces.get(\n",
        "    name=ml_client.workspace_name\n",
        ").mlflow_tracking_uri\n",
        "\n",
        "print(MLFLOW_TRACKING_URI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Set the MLFLOW TRACKING URI\n",
        "\n",
        "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
        "\n",
        "print(\"\\nCurrent tracking uri: {}\".format(mlflow.get_tracking_uri()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 7.1 Get the AutoML parent Job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1663892374471
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from mlflow.tracking.client import MlflowClient\n",
        "\n",
        "# Initialize MLFlow client\n",
        "mlflow_client = MlflowClient()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1663892377506
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#job_name = returned_job.name\n",
        "\n",
        "# Example if providing an specific Job name/ID\n",
        "job_name = \"busy_street_1zqqwj9y3m\"\n",
        "\n",
        "# Get the parent run\n",
        "mlflow_parent_run = mlflow_client.get_run(job_name)\n",
        "\n",
        "print(\"Parent Run: \")\n",
        "print(mlflow_parent_run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1663892380908
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Print parent run tags. 'automl_best_child_run_id' tag should be there.\n",
        "print(mlflow_parent_run.data.tags)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1663892384903
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Get the best model's child run\n",
        "\n",
        "best_child_run_id = mlflow_parent_run.data.tags[\"automl_best_child_run_id\"]\n",
        "print(\"Found best child run id: \", best_child_run_id)\n",
        "\n",
        "best_run = mlflow_client.get_run(best_child_run_id)\n",
        "\n",
        "print(\"Best child run: \")\n",
        "print(best_run)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 7.2 Get best model run's metrics\n",
        "Access the results (such as Models, Artifacts, Metrics) of a previously completed AutoML Run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1663892392808
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(best_run.data.metrics, index=[0]).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 8. Deploy the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 8.1. Create endpoint name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1663892455804
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Creating a unique endpoint name with current datetime to avoid conflicts\n",
        "import datetime\n",
        "\n",
        "online_endpoint_name = \"trash-granular-\" + datetime.datetime.now().strftime(\n",
        "    \"%m%d%Y\")\n",
        "\n",
        "# create an online endpoint\n",
        "endpoint = ManagedOnlineEndpoint(\n",
        "    name=online_endpoint_name,\n",
        "    description=\"this is a sample online endpoint for deploying model\",\n",
        "    auth_mode=\"key\",\n",
        "    tags={\"foo\": \"bar\"},\n",
        ")\n",
        "print(online_endpoint_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1663892590145
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "ml_client.begin_create_or_update(endpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 8.2 Deploy the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1663892593137
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# import required libraries\n",
        "from azure.ai.ml.entities import (\n",
        "    ManagedOnlineEndpoint,\n",
        "    ManagedOnlineDeployment,\n",
        "    Model,\n",
        "    Environment,\n",
        "    CodeConfiguration,\n",
        "    ProbeSettings,\n",
        ")\n",
        "\n",
        "# deploying the mlflow-model\n",
        "model_name = \"trash_detection_big_granular\"\n",
        "model = Model(\n",
        "    path=f\"azureml://jobs/{best_run.info.run_id}/outputs/artifacts/outputs/mlflow-model/\",\n",
        "    name=model_name,\n",
        "    description=\"my trash instance segmentation model\",\n",
        "    type=AssetTypes.MLFLOW_MODEL,\n",
        ")\n",
        "\n",
        "# for downloaded file\n",
        "# model = Model(\n",
        "#     path=path=\"artifact_downloads/outputs/mlflow-model/\",\n",
        "#     name=model_name,\n",
        "#     description=\"my sample instance segmentation model\",\n",
        "#     type=AssetTypes.MLFLOW_MODEL,\n",
        "# )\n",
        "\n",
        "registered_model = ml_client.models.create_or_update(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1663892593478
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "print(registered_model.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1663893713670
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "online_endpoint_name = endpoint.name\n",
        "deployment_name = \"trash-segmentation-granular\"\n",
        "\n",
        "deployment = ManagedOnlineDeployment(\n",
        "    name=deployment_name,\n",
        "    endpoint_name=online_endpoint_name,\n",
        "    model=registered_model.id,\n",
        "    instance_type=\"Standard_DS3_V2\",\n",
        "    instance_count=1,\n",
        "    liveness_probe=ProbeSettings(\n",
        "        failure_threshold=30,\n",
        "        success_threshold=1,\n",
        "        timeout=2,\n",
        "        period=10,\n",
        "        initial_delay=2000,\n",
        "    ),\n",
        "    readiness_probe=ProbeSettings(\n",
        "        failure_threshold=10,\n",
        "        success_threshold=1,\n",
        "        timeout=10,\n",
        "        period=10,\n",
        "        initial_delay=2000,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1663896205391
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# This may take 30 minutes\n",
        "ml_client.online_deployments.begin_create_or_update(deployment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1663896241607
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# is fridge items deployment to take 100% traffic\n",
        "endpoint.traffic = {deployment_name: 100}\n",
        "ml_client.begin_create_or_update(endpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 8.3. Get endpoint details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1663896774909
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Get the details for online endpoint\n",
        "#online_endpoint_name = \"trash-segmentation-big-09202022\"\n",
        "#deployment_name = \"trash-segmentation-mlflow-dpl\"\n",
        "\n",
        "endpoint = ml_client.online_endpoints.get(name=online_endpoint_name)\n",
        "\n",
        "# existing traffic details\n",
        "print(endpoint.traffic)\n",
        "\n",
        "# Get the scoring URI\n",
        "print(endpoint.scoring_uri)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 9. Test Deployment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1663897067702
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import base64\n",
        "\n",
        "\n",
        "def read_image(image_path):\n",
        "    with open(image_path, \"rb\") as f:\n",
        "        return f.read()\n",
        "        \n",
        "sample_image = \"<path to local test image>\"\n",
        "\n",
        "request_json = {\n",
        "    \"input_data\": \n",
        "    {\n",
        "        \"columns\": [\"image\"],\n",
        "        \"data\": [base64.encodebytes(read_image(sample_image)).decode(\"utf-8\")],\n",
        "    }\n",
        "}\n",
        "\n",
        "request_file_name = \"sample_request_data.json\"\n",
        "\n",
        "with open(request_file_name, \"w\") as request_file:\n",
        "    json.dump(request_json, request_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1663897073057
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Get the details for online endpoint\n",
        "#endpoint = ml_client.online_endpoints.get(name=online_endpoint_name)\n",
        "\n",
        "\n",
        "resp = ml_client.online_endpoints.invoke(\n",
        "    endpoint_name=endpoint.name,\n",
        "    deployment_name=deployment_name,\n",
        "    #deployment_name=\"trash-segmentation-big-untuned\",\n",
        "    request_file= request_file_name,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## 10. Visualise Detections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 10.1. Polygon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1663897078304
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.patches as patches\n",
        "from matplotlib.lines import Line2D\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "IMAGE_SIZE = (18, 12)\n",
        "plt.figure(figsize=IMAGE_SIZE)\n",
        "img_np = mpimg.imread(sample_image)\n",
        "img = Image.fromarray(img_np.astype(\"uint8\"), \"RGB\")\n",
        "x, y = img.size\n",
        "\n",
        "fig, ax = plt.subplots(1, figsize=(15, 15))\n",
        "# Display the image\n",
        "ax.imshow(img_np)\n",
        "\n",
        "# draw box and label for each detection\n",
        "detections = json.loads(resp)[0]\n",
        "for detect in detections[\"boxes\"]:\n",
        "    label = detect[\"label\"]\n",
        "    box = detect[\"box\"]\n",
        "    polygon = detect[\"polygon\"]\n",
        "    conf_score = detect[\"score\"]\n",
        "    if conf_score > 0.6: # ADjUST CONFIDENCE SCORE TO GET MORE RESULTS. THIS THRESHOLD FILTERS THE RESULTS AND ONLY KEEPS HIGH CONFIDENCE PREDICTIONS\n",
        "        ymin, xmin, ymax, xmax = (\n",
        "            box[\"topY\"],\n",
        "            box[\"topX\"],\n",
        "            box[\"bottomY\"],\n",
        "            box[\"bottomX\"],\n",
        "        )\n",
        "        topleft_x, topleft_y = x * xmin, y * ymin\n",
        "        width, height = x * (xmax - xmin), y * (ymax - ymin)\n",
        "        print(\n",
        "            \"{}: [{}, {}, {}, {}], {}\".format(\n",
        "                detect[\"label\"],\n",
        "                round(topleft_x, 3),\n",
        "                round(topleft_y, 3),\n",
        "                round(width, 3),\n",
        "                round(height, 3),\n",
        "                round(conf_score, 3),\n",
        "            )\n",
        "        )\n",
        "\n",
        "        color = np.random.rand(3)  #'red'\n",
        "        rect = patches.Rectangle(\n",
        "            (topleft_x, topleft_y),\n",
        "            width,\n",
        "            height,\n",
        "            linewidth=2,\n",
        "            edgecolor=color,\n",
        "            facecolor=\"none\",\n",
        "        )\n",
        "\n",
        "        ax.add_patch(rect)\n",
        "        plt.text(topleft_x, topleft_y - 10, label, color=color, fontsize=20)\n",
        "\n",
        "        polygon_np = np.array(polygon[0])\n",
        "        polygon_np = polygon_np.reshape(-1, 2)\n",
        "        polygon_np[:, 0] *= x\n",
        "        polygon_np[:, 1] *= y\n",
        "        poly = patches.Polygon(polygon_np, True, facecolor=color, alpha=0.4)\n",
        "        ax.add_patch(poly)\n",
        "        poly_line = Line2D(\n",
        "            polygon_np[:, 0],\n",
        "            polygon_np[:, 1],\n",
        "            linewidth=2,\n",
        "            marker=\"o\",\n",
        "            markersize=8,\n",
        "            markerfacecolor=color,\n",
        "        )\n",
        "        ax.add_line(poly_line)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1663767689251
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "detections.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### 10.2. Bounding Box"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1663679379611
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "IMAGE_SIZE = (18, 12)\n",
        "plt.figure(figsize=IMAGE_SIZE)\n",
        "img_np = mpimg.imread(sample_image)\n",
        "img = Image.fromarray(img_np.astype(\"uint8\"), \"RGB\")\n",
        "x, y = img.size\n",
        "\n",
        "fig, ax = plt.subplots(1, figsize=(15, 15))\n",
        "# Display the image\n",
        "ax.imshow(img_np)\n",
        "\n",
        "# draw box and label for each detection\n",
        "detections = json.loads(resp)\n",
        "for detect in detections[0][\"boxes\"]:\n",
        "    label = detect[\"label\"]\n",
        "    box = detect[\"box\"]\n",
        "    conf_score = detect[\"score\"]\n",
        "    if conf_score > 0.6:\n",
        "        ymin, xmin, ymax, xmax = (\n",
        "            box[\"topY\"],\n",
        "            box[\"topX\"],\n",
        "            box[\"bottomY\"],\n",
        "            box[\"bottomX\"],\n",
        "        )\n",
        "        topleft_x, topleft_y = x * xmin, y * ymin\n",
        "        width, height = x * (xmax - xmin), y * (ymax - ymin)\n",
        "        print(\n",
        "            \"{}: [{}, {}, {}, {}], {}\".format(\n",
        "                detect[\"label\"],\n",
        "                round(topleft_x, 3),\n",
        "                round(topleft_y, 3),\n",
        "                round(width, 3),\n",
        "                round(height, 3),\n",
        "                round(conf_score, 3),\n",
        "            )\n",
        "        )\n",
        "\n",
        "        color = np.random.rand(3)  #'red'\n",
        "        rect = patches.Rectangle(\n",
        "            (topleft_x, topleft_y),\n",
        "            width,\n",
        "            height,\n",
        "            linewidth=3,\n",
        "            edgecolor=color,\n",
        "            facecolor=\"none\",\n",
        "        )\n",
        "\n",
        "        ax.add_patch(rect)\n",
        "        plt.text(topleft_x, topleft_y - 10, label, color=color, fontsize=20)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "7013818eacae818f3940355a4bc23a5b5e4daa7053faee37a40d29ac79fa390b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
